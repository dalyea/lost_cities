{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4408281-2787-40fb-9f04-f78bc9f93220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lost Cities V1E\n",
    "# Finished 3 grid searches\n",
    "# Using best determined configuration, long training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f1080e-db86-41cb-8856-a86311492b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import V1E_main as main\n",
    "from V1E_main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f5a96c-24fc-478e-a709-1593f45768fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL PARAMETERS\n",
    "\n",
    "nn_layer_1=64\n",
    "nn_layer_2=32\n",
    "nn_layer_2_dropout=0.20\n",
    "learning_rate=0.001\n",
    "replay_size=20000\n",
    "num_episodes = 200_000\n",
    "batch_size = 64\n",
    "batch_cnt = 3\n",
    "train_every = 3\n",
    "step_booster = 8.0\n",
    "episode_booster = 0.10\n",
    "\n",
    "# Fixed parameters (not in grid)\n",
    "fixed_params = {\n",
    "    'epsilon': 0.30,\n",
    "    'epsilon_min': 0.05,\n",
    "    'epsilon_decay': 0.9999\n",
    "}\n",
    "\n",
    "# Simply comment out any functions not to be included\n",
    "step_functions=[\n",
    "'lower_val_avail',\n",
    "'too_few_pts',\n",
    "'blocked_7',\n",
    "'exp_small_deck',\n",
    "'exp_was_live',\n",
    "'good_exp',\n",
    "'bad_X',\n",
    "'bad_bigger_val',\n",
    "'good_low_val',\n",
    "'draw_to_tgt',\n",
    "# 'had_X',\n",
    "'next_value',\n",
    "# 'bad_center',\n",
    "# 'smart_opp_center'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c1d284-41ab-42ae-945b-c7ea70d062bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modified Training Loop with Action + Draw Selection from Policy\n",
    "def train_model(fv, reward_params, nn_layer_1, nn_layer_2_dropout,\n",
    "                learning_rate, replay_size, step_booster, episode_booster,\n",
    "                step_functions, file_name):\n",
    "\n",
    "    global epsilon, epsilon_min, epsilon_decay\n",
    "    global batch_size, train_every, batch_cnt\n",
    "\n",
    "    global all_rewards\n",
    "    global mean_rewards\n",
    "    all_rewards = []\n",
    "    mean_rewards = []\n",
    "\n",
    "    env = LostCitiesEnv()\n",
    "    num_card_actions = card_cnt\n",
    "    num_draw_choices = color_cnt+1\n",
    "    model = ActorCritic(state_size=state_size, action_size=num_card_actions, draw_size=num_draw_choices, \n",
    "                             nn_layer_1=nn_layer_1, nn_layer_2=nn_layer_2, nn_layer_2_dropout=nn_layer_2_dropout)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    replay_buffer = deque(maxlen=replay_size)\n",
    "    rule_counter = defaultdict(int)\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        mean_reward = 0.0\n",
    "        play_cnt=0\n",
    "        plays_p1 = []\n",
    "        plays_p2 = []\n",
    "    \n",
    "        while not done:\n",
    "            play_cnt+=1\n",
    "            features_np = extract_features(state)\n",
    "            features = torch.tensor(features_np, dtype=torch.float32)\n",
    "    \n",
    "            # Forward pass through the model\n",
    "            card_logits, draw_logits, value = model(features)\n",
    "    \n",
    "            # Get legal actions and legal draws\n",
    "            # actions, draws = env.get_legal_actions(state['current_player'])\n",
    "            actions, draws = env.get_legal_actions(state['current_player'])\n",
    "            legal_action_indices = list(range(len(actions)))\n",
    "    \n",
    "            valid_draws = [d for d in draws if d == 'deck' or (d in env.center_piles and env.center_piles[d])]\n",
    "            if not valid_draws:\n",
    "                print(\"No valid drawsâ€”forcing episode end.\")\n",
    "                done = True\n",
    "                break\n",
    "    \n",
    "            # legal_draw_indices = list(range(len(valid_draws)))  # typically ['deck', 'R', 'B', 'G']\n",
    "    \n",
    "            # Stop this game is no legal actions - though this is of course not true, but...\n",
    "            # Print discard and draw for plays 1001 to 1020\n",
    "            if 10001 <= play_cnt <= 10020:\n",
    "                print(f\"Legal actions: {actions}\")\n",
    "                print(f\"Action indices: {[i for i in range(len(actions))]}\")\n",
    "                # Not valid at this point\n",
    "                # print(f\"Play {play_cnt}: Discard action = {chosen_action}, Draw choice = {chosen_draw}\")\n",
    "            if play_cnt>=10020:\n",
    "                print(play_cnt, actions, draws, valid_draws)\n",
    "                print(f\"\\n--- STUCK STATE at play {play_cnt} ---\")\n",
    "                print(f\"Deck size: {state['deck_size']}\")\n",
    "                print(f\"Player hand: {state['hands'][state['current_player']]}\")\n",
    "                print(f\"Expeditions:\")\n",
    "                for color in env.expeditions[state['current_player']]:\n",
    "                    print(f\"  {color}: {env.expeditions[state['current_player']][color]}\")\n",
    "                print(f\"Center piles:\")\n",
    "                for color in env.center_piles:\n",
    "                    print(f\"  {color}: {env.center_piles[color]}\")\n",
    "                print(f\"Available actions: {actions}\")\n",
    "                print(f\"Available draws: {draws}\")\n",
    "                print(f\"----------------------\\n\")\n",
    "                raise SystemExit(f\"STOP\")\n",
    "            \n",
    "            if not actions:\n",
    "                print(f\"No legal actions for player {state['current_player']}. Ending episode early.\")\n",
    "                done = True\n",
    "                break\n",
    "    \n",
    "            # Sample card action with epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                # Random action\n",
    "                card_idx = random.randint(0, len(actions) - 1)\n",
    "            else:\n",
    "                # Model-based action\n",
    "                card_probs = torch.softmax(card_logits[:len(actions)], dim=0)\n",
    "                card_dist = torch.distributions.Categorical(card_probs)\n",
    "                card_idx = card_dist.sample().item()\n",
    "            \n",
    "            chosen_action = actions[card_idx]\n",
    "    \n",
    "            # Filter valid draws based on chosen_action (if it's a center discard)\n",
    "            discard_color = None\n",
    "            if chosen_action[0] == 'center':\n",
    "                discard_color = chosen_action[1][0]\n",
    "            \n",
    "            filtered_draws = [\n",
    "                d for d in valid_draws if d != discard_color\n",
    "            ]\n",
    "            if not filtered_draws:\n",
    "                # Failsafe: fallback to deck\n",
    "                filtered_draws = ['deck']\n",
    "    \n",
    "            valid_draws=filtered_draws\n",
    "    \n",
    "            # Sample draw choice (FIXED)\n",
    "            if random.random() < epsilon:\n",
    "                chosen_draw = random.choice(valid_draws)\n",
    "            else:\n",
    "                # Correct mapping: get logits only for valid draws\n",
    "                draw_indices_in_logits = [draw_to_index[d] for d in valid_draws]\n",
    "                draw_logits_filtered = draw_logits[draw_indices_in_logits]\n",
    "                draw_probs = torch.softmax(draw_logits_filtered, dim=0)\n",
    "                draw_dist = torch.distributions.Categorical(draw_probs)\n",
    "                draw_idx = draw_dist.sample().item()\n",
    "                chosen_draw = valid_draws[draw_idx]\n",
    "    \n",
    "            # Compute shaped intermediate reward\n",
    "            step_reward = compute_step_reward_grid(state, chosen_action, chosen_draw, env, step_functions, reward_params, rule_counter)\n",
    "    \n",
    "            # Map draw_choice to its index for policy update\n",
    "            chosen_draw_idx = draw_to_index[chosen_draw]\n",
    "    \n",
    "            # Save the current player before doing env.step\n",
    "            current_player=state['current_player']\n",
    "            \n",
    "            # Take action and draw based on policies\n",
    "            next_state, reward, done = env.step(chosen_action, chosen_draw)\n",
    "    \n",
    "            # Combine shaped reward + final score (if any)\n",
    "            # total_reward = reward + booster * step_reward\n",
    "            total_reward = step_booster * step_reward\n",
    "    \n",
    "            # Store full experience (must include both action idx and draw idx!)\n",
    "            # replay_buffer.append((features_np, card_idx, chosen_draw_idx, total_reward))\n",
    "            # Now, do it all at end of game\n",
    "            if current_player=='P1':\n",
    "                plays_p1.append((features_np, card_idx, chosen_draw_idx, step_reward))\n",
    "            else:\n",
    "                plays_p2.append((features_np, card_idx, chosen_draw_idx, step_reward))\n",
    "                \n",
    "            # Advance state\n",
    "            state = next_state\n",
    "            mean_reward += total_reward\n",
    "    \n",
    "            ddebug = random.random()<1e-6\n",
    "            if done:\n",
    "                reward_p1 = env.compute_score('P1')\n",
    "                reward_p2 = env.compute_score('P2')\n",
    "                p1cnt=0\n",
    "                for (features_np, card_idx, draw_idx, step_reward) in plays_p1:\n",
    "                    total_reward = episode_booster * reward_p1 + step_reward\n",
    "                    replay_buffer.append((features_np, card_idx, draw_idx, total_reward))\n",
    "                    if ddebug:\n",
    "                        p1cnt+=1\n",
    "                        print(f\"P1 {p1cnt} - {episode_booster} * {reward_p1} + {step_reward} = {total_reward}\")\n",
    "                for (features_np, card_idx, draw_idx, step_reward) in plays_p2:\n",
    "                    total_reward = episode_booster * reward_p2 + step_reward\n",
    "                    replay_buffer.append((features_np, card_idx, draw_idx, total_reward))\n",
    "    \n",
    "        # Final mean reward is the average over plays - approximate over P1 and P2\n",
    "        mean_reward=1.0*mean_reward/play_cnt\n",
    "\n",
    "        # Annealing\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "        if play_cnt>200:\n",
    "            print(f\"Plays: {play_cnt} in episode {episode}\")\n",
    "    \n",
    "        # Train\n",
    "        if episode % train_every == 0 and len(replay_buffer) >= batch_size:\n",
    "            for _ in range(batch_cnt):\n",
    "                minibatch = random.sample(replay_buffer, batch_size)\n",
    "        \n",
    "                # Unpack minibatch into separate lists\n",
    "                states_b, actions_b, draws_b, rewards_b = zip(*minibatch)\n",
    "    \n",
    "                # Convert lists to tensors\n",
    "                states_np = np.array(states_b)  # Convert list of arrays â†’ single array\n",
    "                states_t = torch.tensor(states_np, dtype=torch.float32)\n",
    "        \n",
    "                # Convert to tensors in batch\n",
    "                # states_t = torch.tensor(states_b, dtype=torch.float32)  # Shape: [batch_size, state_size]\n",
    "                actions_t = torch.tensor(actions_b, dtype=torch.long)   # Shape: [batch_size]\n",
    "                draws_t = torch.tensor(draws_b, dtype=torch.long)       # Shape: [batch_size]\n",
    "                rewards_t = torch.tensor(rewards_b, dtype=torch.float32)  # Shape: [batch_size]\n",
    "        \n",
    "                # Forward pass in batch\n",
    "                card_logits_b, draw_logits_b, values_b = model(states_t)  # Each output shape: [batch_size, num_actions/draws]\n",
    "        \n",
    "                # Compute log probs for card actions\n",
    "                card_probs_b = torch.softmax(card_logits_b, dim=1)\n",
    "                log_card_probs_b = torch.log(card_probs_b + 1e-8)\n",
    "                selected_log_card_probs = log_card_probs_b[range(batch_size), actions_t]\n",
    "        \n",
    "                # Compute log probs for draws\n",
    "                draw_probs_b = torch.softmax(draw_logits_b, dim=1)\n",
    "                log_draw_probs_b = torch.log(draw_probs_b + 1e-8)\n",
    "                selected_log_draw_probs = log_draw_probs_b[range(batch_size), draws_t]\n",
    "        \n",
    "                # Compute advantage\n",
    "                advantages = rewards_t - values_b.squeeze(1)  # Shape: [batch_size]\n",
    "        \n",
    "                # Losses\n",
    "                critic_loss = advantages.pow(2).mean()\n",
    "                actor_loss_card = -(selected_log_card_probs * advantages).mean()\n",
    "                actor_loss_draw = -(selected_log_draw_probs * advantages).mean()\n",
    "        \n",
    "                total_loss = critic_loss + actor_loss_card + actor_loss_draw\n",
    "        \n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "        all_rewards.append(reward_p1)\n",
    "        all_rewards.append(reward_p2)    \n",
    "        mean_rewards.append(mean_reward)\n",
    "    \n",
    "        if episode % 5000 == 0:\n",
    "            print(\"\\n=== Step Rule Firing Counts ===\")\n",
    "            for rule, count in sorted(rule_counter.items(), key=lambda x: -x[1]):\n",
    "                print(f\"{rule:<30}: {count}\")\n",
    "            pd.Series(all_rewards).to_csv(file_name, index=False, header=False)\n",
    "        \n",
    "        if episode % 500 == 0:\n",
    "            avg_score = np.mean(all_rewards[-2000:]) if len(all_rewards) >= 2000 else np.mean(all_rewards)\n",
    "            print(f\"Episode {episode}, Average Reward Last {min(len(all_rewards), 1000)}: {avg_score:.2f}, eps={epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf947d1-860e-448e-b422-e582f89b3c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs: 1\n",
      "Configuration saved to config_100_grid_00_141038.txt\n",
      "Episode 500, Average Reward Last 1000: -0.14, eps=0.2440\n",
      "Episode 1000, Average Reward Last 1000: 0.03, eps=0.2385\n",
      "Episode 1500, Average Reward Last 1000: -0.19, eps=0.2328\n",
      "Episode 2000, Average Reward Last 1000: -0.44, eps=0.2272\n",
      "Episode 2500, Average Reward Last 1000: -0.41, eps=0.2215\n",
      "Episode 3000, Average Reward Last 1000: -0.62, eps=0.2160\n",
      "Episode 3500, Average Reward Last 1000: -0.74, eps=0.2107\n",
      "Episode 4000, Average Reward Last 1000: -0.55, eps=0.2047\n",
      "Episode 4500, Average Reward Last 1000: -0.40, eps=0.1977\n",
      "\n",
      "=== Step Rule Firing Counts ===\n",
      "good_exp_1                    : 27718\n",
      "draw_to_tgt                   : 21729\n",
      "too_few_pts                   : 18669\n",
      "good_exp                      : 17693\n",
      "good_low_val                  : 15089\n",
      "bad_center                    : 11363\n",
      "next_value                    : 9835\n",
      "lower_val_avail               : 4186\n",
      "bad_bigger_val                : 4186\n",
      "bad_X                         : 2404\n",
      "Episode 5000, Average Reward Last 1000: -0.39, eps=0.1915\n",
      "Episode 5500, Average Reward Last 1000: -0.11, eps=0.1856\n",
      "Episode 6000, Average Reward Last 1000: 0.08, eps=0.1797\n",
      "Episode 6500, Average Reward Last 1000: 0.33, eps=0.1746\n",
      "Bad center ('center', 'R6') holding ['G3', 'R3', 'R6']\n",
      "Episode 7000, Average Reward Last 1000: 0.64, eps=0.1701\n",
      "Episode 7500, Average Reward Last 1000: 0.92, eps=0.1658\n",
      "Episode 8000, Average Reward Last 1000: 0.97, eps=0.1615\n",
      "Episode 8500, Average Reward Last 1000: 0.70, eps=0.1571\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "running=100\n",
    "\n",
    "grid = {\n",
    "    \"nn_layer_1\": [64],\n",
    "    \"nn_layer_2_dropout\": [0.1],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"replay_size\": [20000],\n",
    "    \"step_booster\": [8.0],\n",
    "    \"episode_booster\": [0.1],\n",
    "    \"step_functions\": [\n",
    "        ['good_exp', 'draw_to_tgt', 'too_few_pts', 'good_low_val', 'next_value','bad_bigger_val','bad_X',\n",
    "         'lower_val_avail','exp_was_live','exp_small_deck','bad_center','next_value']\n",
    "    ]\n",
    "}\n",
    "\n",
    "# This has all step rewards to explore and evaluate\n",
    "reward_grid = {\n",
    "    \"good_exp\":        [0.3],\n",
    "    \"good_exp_1\":      [1.5],    \n",
    "    \"draw_to_tgt\":     [1.5],\n",
    "    \"too_few_pts\":     [-0.3],\n",
    "    \"bad_X\":           [-1.5], # was -2.0\n",
    "    \"bad_center\":      [-1.5], # was -2.0\n",
    "    \"good_low_val\":    [1.5],  # was +2.0\n",
    "    \"bad_bigger_val\":  [-1.5], # was -2.0\n",
    "    \"next_value\":      [0.3],\n",
    "    \"blocked_7\":       [-0.3],\n",
    "    \"lower_val_avail\": [-1.5]\n",
    "}\n",
    "\n",
    "# Create Cartesian product of all combinations\n",
    "keys, values = zip(*grid.items())\n",
    "combinations = list(product(*values))\n",
    "\n",
    "print(f\"Total runs: {len(combinations)}\")\n",
    "\n",
    "# Main loop\n",
    "for idx, combo in enumerate(combinations):\n",
    "    params = dict(zip(keys, combo))\n",
    "    fv = f\"grid_{idx:02d}_{datetime.datetime.now().strftime('%H%M%S')}\"\n",
    "    file_name = f\"all_rewards.{running}.{fv}.csv\"\n",
    "\n",
    "    # Add file_name to params so it can be passed to train_model\n",
    "    params['file_name'] = file_name\n",
    "  \n",
    "    # Save config file\n",
    "    save_config_txt(running, fv, params, params['step_functions'])\n",
    "\n",
    "    # Variables to reset\n",
    "    epsilon = fixed_params['epsilon']\n",
    "    epsilon_min = fixed_params['epsilon_min']\n",
    "    epsilon_decay = fixed_params['epsilon_decay']\n",
    "\n",
    "    # Train grid\n",
    "    # reward_params=reward_grid.copy()\n",
    "    reward_params = {k: v[0] for k, v in reward_grid.items()}\n",
    "    train_model(fv, reward_params, **params)\n",
    "\n",
    "    # Optional: log progress\n",
    "    print(f\"Finished run {idx+1}/{len(combinations)} â†’ {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "81139b17-fc42-47cf-bde3-448596ffd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'lc_model_'+fv+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04655e75-7777-4871-adbb-f9abc72cbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic(\n",
       "  (fc1): Linear(in_features=43, out_features=96, bias=True)\n",
       "  (fc2): Linear(in_features=96, out_features=32, bias=True)\n",
       "  (dropout): Dropout(p=0.15, inplace=False)\n",
       "  (policy_action_head): Linear(in_features=32, out_features=18, bias=True)\n",
       "  (policy_draw_head): Linear(in_features=32, out_features=4, bias=True)\n",
       "  (value_head): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('lc_model_v1B_2.pt'))\n",
    "# model.eval()  # Important: sets model to evaluation mode (no dropout etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
